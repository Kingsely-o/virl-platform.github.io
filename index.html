<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script src="cross_fade.js"></script>
        <script defer="" src="hider.js"></script>
        <script src="image_interact.js"></script>
        <script src="switch_videos.js"></script>
        <script src="my_magnifier.js"></script>

        <link rel="stylesheet" href="style.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1 style="margin-top: 0px"><em>V-ILR</em>: Grounding Virtual Intelligence in Real Life</h1>
              <p style="color: #FFF7D4">We build a platform that enables agents to scalably interact with the real-world in a virtual yet realistic environment.</p>
            </div>
            <div class="header-image">
                <img src="virl_images/teaser_img_v2.png" alt="Teaser Image" class="teaser-image">
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://jihanyang.github.io/" class="author-link">Jihan Yang</a></p>
                    <p><a href="https://dingry.github.io/" class="author-link">Runyu Ding</a></p>
                    <p><a href="https://ellisbrown.github.io/" class="author-link">Ellis Brown</a></p>
                    <p><a href="https://xjqi.github.io/" class="author-link">Xiaojuan Qi</a></p>
                    <p><a href="https://www.sainingxie.com/" class="author-link">Saining Xie</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                </div>
                <div class="byline-column">
                    <h3>Code</h3>
                    <p><a href="#">Github</a></p>
                </div>
                <div class="byline-column">
                    <h3>Paper</h3>
                    <p><a href="#">Arxiv</a></p>
                </div>
            </div>
        </div>

        <d-contents style="margin-top: 5px; position: relative;">
            <nav>
                <h4>Contents</h4>
                <div><a href="#agent-examplers">Virtual Intelligence in Real Life</a></div>
                <ul>
                    <li><a href="#agent-earthbound" style="font-size: 12px;">Earthbound Agents</a></li>
                    <li><a href="#agent-language" style="font-size: 12px;">Language-Driven Agents</a></li>
                    <li><a href="#agent-vision" style="font-size: 12px;">Visually Grounded Agents</a></li>
                    <li><a href="#agent-agent-col" style="font-size: 12px;">Agent-Agent Collaboration</a></li>
                    <li><a href="#agent-human-col" style="font-size: 12px;">Human-Agent Collaboration</a></li>
                </ul>
                <div><a href="#system">System Fundamental</a></div>
                <div><a href="#virlbenchmark"><em>V-IRL</em> Benchmark</a></div>
            </nav>
        </d-contents>

        <p>
        There is a sensory gulf between the Earth that humans inhabit and the imaginary, digital realms in which modern AI agents are created.
        To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it's essential to bridge the gap between the digital and physical worlds.
        How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control?
        Towards this end, we introduce <strong><i>V-IRL</i></strong>: a platform that enables agents to scalably interact with the real-world in a virtual yet realistic environment.
        Our platorm serves as a playground for developing agents that can accomplish a variety of practical tasks, and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data spanning the entire globe.
        </p>
        <div class="l-page teaser-video">
            <video autoplay loop muted>
                <source src="videos/teaser_video.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>

        <h2 id="agent-examplers">Virtual Intelligence in Real Life</h2>
        <p>
            To show the versatility of the <strong><i>V-IRL</i></strong> platform, we use it to instantiate several exemplar agents in our virtual real-world environment. For illustration, we give <strong><i>V-IRL</i></strong> agents character metadata, including an 8-bit avatar, a name, a short bio, and an intention they are trying to accomplish.
        </p>

        <h3 id="agent-earthbound">Earthbound Agents</h3>
        <p>
        Agents in the <i>V-IRL</i> platform inhabit virtual representations of real cities around the globe. At the core of this representation are <em>geographic coordinates</em> corresponding to points on the Earth's surface. Using these coordinates, <i>V-IRL</i> allows agents to <i>ground</i> themselves in the world using maps, <em>real</em> street view imagery, information about nearby destinations, and additional data from arbitrary geospatial APIs
        </p>
        <p class="l-body">
            <img src="character_card/courier.png" class="character-card-img">
        </p>
        <blockquote> 
            Peng needs to visit several locations throughout the city to get documents signed for registration as a visiting student...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Leveraging Geolocation & Mapping capabilities, Peng saves 7 minutes by 
            walking along the shortest path as opposed to in order waypoint visitation.
        </p>
        
        <div class="collapsible"><h4>AGENT BEHAVIOR EXAMPLE <svg height="25px" width="25px" version="1.1" id="_x32_" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="-51.2 -51.2 614.40 614.40" xml:space="preserve" fill="#000000" stroke="#000000"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round" stroke="#CCCCCC" stroke-width="1.024"></g><g id="SVGRepo_iconCarrier"> <style type="text/css"> .st0{fill:#000000;} </style> <g> <path class="st0" d="M278.796,94.952c26.218,0,47.472-21.254,47.472-47.481C326.268,21.254,305.014,0,278.796,0 c-26.227,0-47.481,21.254-47.481,47.472C231.315,73.698,252.569,94.952,278.796,94.952z"></path> <path class="st0" d="M407.86,236.772l-54.377-28.589l-22.92-47.087c-11.556-23.754-33.698-40.612-59.679-45.439l-23.58-4.386 c-11.859-2.197-24.111-0.614-35.027,4.542l-68.67,32.426c-7.628,3.599-13.654,9.863-16.969,17.601l-30.539,71.308 c-1.941,4.533-1.978,9.652-0.11,14.202c1.868,4.561,5.494,8.187,10.046,10.055l0.686,0.275c9.102,3.726,19.532-0.384,23.654-9.314 l28.03-60.704l44.368-14.34l-43.964,195.39l-42.82,106.765c-2.372,5.916-2.106,12.555,0.715,18.26 c2.82,5.714,7.938,9.954,14.074,11.667l1.85,0.512c9.844,2.747,20.293-1.511,25.42-10.357l50.751-87.663l30.237-59.998 l55.182,60.896l40.76,86.354c4.596,9.734,15.466,14.834,25.887,12.133l0.458-0.128c6.053-1.566,11.163-5.586,14.13-11.09 c2.94-5.504,3.47-11.996,1.438-17.903l-29.99-86.93c-4.212-12.225-10.457-23.644-18.47-33.79l-48.699-64.394l17.866-92.92 l23.058,29.294c2.848,3.626,6.538,6.52,10.741,8.426l60.658,27.388c4.387,1.979,9.387,2.098,13.864,0.33 c4.479-1.768,8.05-5.274,9.9-9.716l0.192-0.467C419.562,250.874,416.019,241.067,407.86,236.772z"></path> </g> </g></svg></h4> <div class="collapsible-indicator"></div></div>
        <div class="content">
            <d-figure>
                <figure style="margin-bottom: 10px">
                    <img src="virl_images/courier.png" alt="Route optimizer figure">
                    <figcaption>Finding the shortest path for Peng to travel to five places.</figcaption>
                </figure>
            </d-figure>
        </div>
        

        <h3 id="agent-language">Language-Driven Agents</h3>
        <p>
            To tackle more complex tasks, we follow the pattern of language-driven agents <d-cite key="xi2023rise"></d-cite>. LLMs
            enable agents to reason, plan  
            and use external tools & APIs.
        </p>
        <p class="l-body">
            <img src="character_card/place_recommender.png" class="character-card-img">
        </p>
        <!-- place recommender agent -->
        <blockquote> 
            Peng is starving for some lunch but doesn't know where to eat...
            Luckily, he met a nice grad student Aria during his errands who might be able to help him find a good spot...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Aria searches for possible restaurants nearby.
            She then synthesizes public reviews to make final recommendations via GPT-4. As Peng is new to the city and originally from Sichuan, she recommends the spicy Chinese joint <em>Chow House 粤德轩</em> to give him a taste of home.
        </p>
        <div class="collapsible"><h4>AGENT BEHAVIOR EXAMPLE <svg width="25px" height="25px" viewBox="0 0 64 64" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--medical-icon" preserveAspectRatio="xMidYMid meet" fill="#000000"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round"></g><g id="SVGRepo_iconCarrier"><path d="M56.411.719S7.623.704 7.637.719C3.14.719.612 3.044.612 7.793v48.966c0 4.443 2.273 6.769 6.766 6.769h49.173c4.493 0 6.769-2.21 6.769-6.769V7.793c.001-4.634-2.275-7.074-6.909-7.074zM30.749 23.374c0 1.536-1.399 3.181-3.215 3.181V52.49c0 3.682-5.018 3.682-5.018 0V26.555c-1.767 0-3.306-1.361-3.306-3.4V8.882c0-1.242 1.795-1.29 1.795.049v10.55h1.503V8.833c0-1.141 1.729-1.214 1.729.049v10.599h1.553V8.848c0-1.193 1.678-1.241 1.678.047v10.586h1.528V8.848c0-1.18 1.753-1.227 1.753.047v14.479zm13.386 29.104c0 3.601-5.028 3.547-5.028 0V36.496H36.43V12.199c0-5.656 7.706-5.656 7.706 0v40.279z" fill="#000000"></path></g></svg></h4> <div class="collapsible-indicator"></div></div>
        <div class="content interactive-image">
            <div id="image-text-container">
            <d-figure>
                <figure style="margin-bottom: 20px;">
                    <img style="width: 60%;" src="virl_images/place_recommend/place1.png" alt="place illustration">
                    <br>
                    <span>Agent Consideration:</span>
                    <figcaption>Chow House is a highly recommended Sichuan restaurant, which aligns with Peng's background as he grew up in Sichuan. The restaurant offers authentic Sichuan food, which Peng might be familiar with and enjoy. The restaurant also has good seating, decoration, and friendly service, which would make for a pleasant dining experience. However, some dishes received mixed reviews, which is why the rating is not a perfect 10.</figcaption>
                </figure>
            </d-figure>
            <p x="165" y="-12.5" text-anchor="left" style="font-weight: 700; font-size: 15px; font-family: sans-serif;">Click to check different candidate places:</p>
            <div class="button-bar">
                <!-- Buttons to interact with the image and text -->
                <button onclick="changeContent('Place1')">Chow House</button>
                <button onclick="changeContent('Place2')">Kwa Food Fried Skewers</button>
                <button onclick="changeContent('Place3')">Tartinery Cafe</button>
                <button onclick="changeContent('Place4')">Sushi Zo</button>
                <button onclick="changeContent('Place5')">Dos Toros Taqueria</button>
            </div>
            </div>
        </div>
        
        <!-- Real estate agent -->
        <blockquote> 
            Peng hires Vivek to help him find an apartment in East Village, Jersey City, or Long Island City for $1k--$3k per month close to a gym, supermarket, and public transit...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Vivek uses real estate APIs to find potential apartments in Peng's desired regions and price range.
            For each candidate, he researches its proximity to the places Peng cares about. Synthesizing these factors, Vivek provides a holistic rating and accompanying reasoning using GPT-4.
            His top recommendation is a cost-effective 1 bedroom apartment for $1986/mo, which is close to a supermarket, 2 bus stations, and a gym.
        </p>

        <div class="collapsible"><h4>AGENT BEHAVIOR EXAMPLE <svg width="25px" height="25px" viewBox="0 0 24.00 24.00" id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" fill="#000000"><g id="SVGRepo_bgCarrier" stroke-width="0"></g><g id="SVGRepo_tracerCarrier" stroke-linecap="round" stroke-linejoin="round" stroke="#CCCCCC" stroke-width="0.048"></g><g id="SVGRepo_iconCarrier"><defs><style>.cls-1{fill:none;stroke:#020202;stroke-miterlimit:10;stroke-width:1.92px;}</style></defs><path class="cls-1" d="M1.46,13.44H2.9a1.44,1.44,0,0,1,1.44,1.44v0A1.44,1.44,0,0,1,2.9,16.31H1.46a0,0,0,0,1,0,0V13.44a0,0,0,0,1,0,0Z"></path><path class="cls-1" d="M1.46,20.15V16.31h1a1.91,1.91,0,0,1,1.91,1.92v1.92"></path><polyline class="cls-1" points="10.08 19.19 7.21 19.19 7.21 13.44 10.08 13.44"></polyline><line class="cls-1" x1="9.13" y1="16.31" x2="7.21" y2="16.31"></line><polyline class="cls-1" points="12 20.15 12 13.44 12.96 13.44 14.88 19.19 15.83 19.19 15.83 12.48"></polyline><line class="cls-1" x1="17.75" y1="13.44" x2="23.5" y2="13.44"></line><line class="cls-1" x1="20.63" y1="20.15" x2="20.63" y2="13.44"></line><line class="cls-1" x1="23.5" y1="22.06" x2="0.5" y2="22.06"></line><polyline class="cls-1" points="22.54 10.56 12 1.94 1.46 10.56"></polyline></g></svg></h4>  <div class="collapsible-indicator"></div> </div>
        <div class="content">
            <d-figure>
                <figure>
                    <img src="virl_images/agent_estate.png" alt="Estate recommender">
                    <figcaption>Part of candidate estates.</figcaption>
                </figure>
            </d-figure>
        </div>
        
        <!-- Vision agents -->
        <h3 id="agent-vision">Visually Grounded Agents</h3>
        <p>
            Although language-driven agents can address some real-world tasks using external tools, their reliance on solely text-based information limits their applicability to tasks where <em>visual grounding</em> is required.
            In contrast, <em>real sensory input</em> is integral to many daily human activities---allowing a deep connection to and understanding of the 
            real world around us.
            Agents can leverage street view imagery through the <i>V-IRL</i> platform to <em>visually ground</em> themselves in the real world---opening up a wide range of <em>perception-driven tasks</em>.
        </p>

        <!-- RX-399 -->
        <p class="l-body">
            <img src="character_card/rx399.png" class="character-card-img">
        </p>
        <blockquote> 
            RX-399 is a state-of-the-art robot agent with advanced navigation and sensing capabilities. Its manufacturer is running a pilot program with sanitation departments in Hong Kong and New York City to assess its readiness for garbage duty...
        </blockquote>
        <hr class="l-body fancybreak">

        <p>
            RX-399 navigates along pre-defined city routes, tagging all trash bins using its open-world detector and geolocation module as depicted in the following figure and videos. 
            <!-- <d-figure>
                <figure>
                    <img src="virl_images/rx-399_clean.png" alt="RX-399 detects trash bin illustration">
                    <figcaption style="text-align: center">Portions of RX-399's system records in New York City and Hong Kong.</figcaption>
                </figure>
            </d-figure> -->
        </p>

        <div id="slider-img-rx399" class="slider-img-container">
            <div class="my-slides">
              <!-- <div class="numbertext">1 / 2</div> -->
              <img src="virl_images/rx-399/rx-399_clean_ny.png" style="width:100%">
            </div>
          
            <div class="my-slides">
              <!-- <div class="numbertext">2 / 2</div> -->
              <img src="virl_images/rx-399/rx-399_clean_hk.png" style="width:100%">
            </div>
              
            <a class="prev" onclick="plusSlides('slider-img-rx399', -1)">❮</a>
            <a class="next" onclick="plusSlides('slider-img-rx399', 1)">❯</a>
            
            <figcaption id="caption" style="margin-bottom: 10px; text-align: center"></figcaption>
          
            <div class="row">
                <div class="column">
                    <img class="demo cursor" src="virl_images/rx-399/rx-399_clean_ny.png" style="width:100%" onclick="currentSlide('slider-img-rx399', 1)" alt="Portions of RX-399's system records in New York City.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/rx-399/rx-399_clean_hk.png" style="width:100%" onclick="currentSlide('slider-img-rx399', 2)" alt="Portions of RX-399's system records in Hong Kong">
                </div>
            </div>
        </div>

        <div class="l-body">
            <div id="RX399video1Container" class="video-container">
                <iframe src="https://www.youtube.com/embed/Q6Hty1BTLdw?si=XAK_3qkchKgrDVit" title="RX-399 in New York City" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div id="RX399video2Container" class="video-container" style="display:none;">
                <iframe src="https://www.youtube.com/embed/9gudvNT5Ces?si=j4K_7ULDvNMIt5dp" title="RX-399 in Hong Kong" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
          
            <!-- Preview Images in a Flex Container -->
            <div class="preview-container">
                <text x="165" y="-12.5" text-anchor="middle" style="font-weight: 700; font-size: 15px; font-family: sans-serif;">Switch recording videos between NYC and HK:</text>
                <img class="preview" src="virl_images/previews/video_rx-399_preview.png" alt="Preview image of RX-399 NYC" onclick="switchVideo('RX399', 'video1Container')">
                <img class="preview" src="virl_images/previews/video_rx-399_hk_preview.png" alt="Preview 2" onclick="switchVideo('RX399','video2Container')">
            </div>
        </div>
        
        
        <!-- Urban Planner -->
        <p class="l-body">
            <img src="character_card/urban_planner.png" class="character-card-img">
        </p>
        <blockquote>
            Imani needs to analyze the distribution of trash bins, fire hydrants, and park benches in New York's Central Park for a project with the NYC Parks & Recreation department...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            Imani sets routes spanning Central Park and objects of interest for RX-399, who traverses the routes and records all detected instances. 
            After RX-399 finishes its route, Imani analyzes the collected data by RX-399 at different levels of detail.
            <!-- As depicted in \cref{fig:agent_urban_plan}, the coarsest level shows general \geoul{distributions} of trash bins, hydrants, and benches in the park. -->
            <!-- Imani can also zoom in to specific regions, where lighter colors represent positions with more unique instances identified.  -->
        </p>
        <div class="img-magnifier-container">
            <img id="urban_planner_img" style="width: 100%" src="virl_images/urban_planner.png" alt="Urban Planner agent visualization">
            <figcaption>Imani's visualization of trash bins, fire hydrants, park benches in NYC's Central Park using data collected by RX-399. The coarsest level shows general distributions of trash bins, hydrants, and benches in the park.
            Imani can also zoom in to specific regions, where lighter colors represent positions with more unique instances identified.</figcaption>
        </div>

        <aside class="counting-table">
            <figure style="width: 240px">
                <table style="margin-bottom: 5px">
                    <tr>
                        <th style="font-size: 11px;">Category</th>
                        <th style="font-size: 11px;">Trash bin</th>
                        <th style="font-size: 11px;">Hydrant</th>
                        <th style="font-size: 11px;">Bench*</th>
                    </tr>
                    <tr>
                        <td style="font-size: 11px;">Count</td>
                        <td style="font-size: 11px;">1059</td>
                        <td style="font-size: 11px;">727</td>
                        <td style="font-size: 11px;">1015</td>
                    </tr>
                </table>
                <figcaption class="table-caption">
                    Table 1: RX-399's counting report. *Note: contiguous benches counted as one instance.
                </figcaption>
            </figure>
        </aside>

        <div class="l-body">
            <div id="UrbanPlannervideo1Container" class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/Tq4fkKXrxYo?si=663Xhkzoq6XOONgF" title="Urban Planner agent video illustration" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div id="UrbanPlannervideo2Container" class="video-container" style="display:none;">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/Tj_3A1vHUZU?si=d78eIoIIBZjPbvEB" title="Play on heatmap" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
          
            <!-- Preview Images in a Flex Container -->
            <div class="preview-container">
                <text x="165" y="-12.5" text-anchor="middle" style="font-weight: 700; font-size: 15px; font-family: sans-serif;">Switch videos between data collecting and heatmap distribution:</text>
                <img id="preview1" class="preview" src="virl_images/previews/video_urban_plan_collect_preview.png" alt="Preview image of urban planner exploration" onclick="switchVideo('UrbanPlanner', 'video1Container')">
                <img id="preview2" class="preview" src="virl_images/previews/video_urban_plan_play_preview.png" alt="Preview image of urban planner checking" onclick="switchVideo('UrbanPlanner','video2Container')">
            </div>
        </div>

        <!-- Intentional explorer -->
        <p class="l-body">
            <img src="character_card/intentional_explorer.png" class="character-card-img">
        </p>
        <blockquote>
            Hiro is starting a new journey in Hong Kong.
            He decides to explore without a specific destination in mind, looking for a good local lunch spot with food that's not too spicy...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            As depicted in the following figure and video, driven by his intention, Hiro uses VQA to select proper roads; uses open-world detection to find a restaurant; uses place reviews and LLM to decide whether a place is suitable for his purpose.
        </p>

        <d-figure>
            <figure>
                <img src="virl_images/intentional_explorer.png" style="width: 100%" alt="Intentional Explorer agent exploring">
                <figcaption style="width: 100%; text-align: center">Visualization for Hiro's lunch exploration in HK. Concrete procedure is depicted in the following video.</figcaption>
            </figure>
        </d-figure>

        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/bOGa7eI-K_I?si=MPPoxbxf8S7Dj7Tp" title="Intentional Explorer agent exploration procedure" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        
        <h3 id="agent-agent-col">Agent-Agent Collaborative Agents</h3>
        <p>
            As with previous agents, collaborative agents are designed for specific tasks; however, they can handle objectives beyond their expertise through collaboration with each other.
        </p>
        
        <p class="l-body">
            <img src="character_card/tourist.png" class="character-card-img">
        </p>
        <blockquote>
            Ling travels to cities around the world. She 
            seeks out authentic experiences and
            is always unafraid to ask for help from Locals whenever she finds herself lost...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            After obtaining route descriptions from Locals, Ling starts her journey. Grounded in our embodied platform, Ling can adjust her pose and identify visual landmarks along the streets using open-world recognition and her map. Recognizing these landmarks helps GPT-4 to make correct decisions about where to turn direction, move forward and stop. Concrete examples are shown in the following figure and videos.
        </p>
        <!-- <d-figure>
            <figure>
                <img src="virl_images/navigation.png" alt="Tourist-Local collaboration" style="width: 100%">
                <figcaption style="width: 100%; text-align: center">Ling and Local collaboration examples. Trajectories in red and green mean Ling's first and second attempts, respectively.</figcaption>
            </figure>
        </d-figure> -->

        <div id="slider-img-tourist" class="slider-img-container">
            <div class="my-slides">
              <!-- <div class="numbertext">1 / 4</div> -->
              <img src="virl_images/tourist/tourist_nyc_1.png" style="width:100%">
            </div>
          
            <div class="my-slides">
              <!-- <div class="numbertext">2 / 4</div> -->
              <img src="virl_images/tourist/tourist_nyc_2.png" style="width:100%">
            </div>

            <div class="my-slides">
                <img src="virl_images/tourist/tourist_sf.png" style="width:100%">
                <!-- <div class="numbertext">3 / 4</div> -->
            </div>

            <div class="my-slides">
                <img src="virl_images/tourist/tourist_hk.png" style="width:100%">
                <!-- <div class="numbertext">4 / 4</div> -->
            </div>
              
            <a class="prev" onclick="plusSlides('slider-img-tourist', -1)">❮</a>
            <a class="next" onclick="plusSlides('slider-img-tourist', 1)">❯</a>
            
            <figcaption id="caption" style="margin-bottom: 10px; text-align: center"></figcaption>
          
            <div class="row">
                <div class="column">
                    <img class="demo cursor" src="virl_images/tourist/tourist_nyc_1.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-tourist', 1)" alt="Ling and Local collaboration examples in New York City. Ling successfully find a nearby gift store by following the route description from Local agent.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/tourist/tourist_nyc_2.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-tourist', 2)" alt="Another Ling and Local collaboration examples in New York City. Ling successfully find a good burger spot by following the route description from Local agent.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/tourist/tourist_sf.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-tourist', 3)" alt="Ling and Local collaboration examples in San Francisco. Ling passes by the destination because only the wall of the Apple store is visible from her viewpoint. Fortunately, she can ask another Local agent nearby to start another round of navigation, which eventually leads her to the destination. Ling's first and second attempts are shown in red and green trajectories, respectively">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/tourist/tourist_hk.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-tourist', 4)" alt="Ling and Local collaboration examples in Hong Kong. Ling mistakes another restaurant as her destination at her first attempt. She then can ask another Local agent nearby to start another round of navigation, which eventually leads her to the destination. Ling's first and second attempts are shown in red and green trajectories, respectively">
                </div>
            </div>
        </div>

        <div class="l-body">
            <div id="Touristvideo1Container" class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/H_F-_NoHDLk?si=7TcYGkiFOIBaCHZo" title="Tourist-Local collaboration in San Francisco" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
            <div id="Touristvideo2Container" class="video-container" style="display:none;">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/kS2zIDyVvSU?si=hoA4oI4sKXWDk1Wy" title="Tourist-Local collaboration in Hong Kong" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
            </div>
          
            <!-- Preview Images in a Flex Container -->
            <div class="preview-container">
                <text x="165" y="-12.5" text-anchor="middle" style="font-weight: 700; font-size: 15px; font-family: sans-serif;">Switch videos between SF and HK journeys:</text>
                <img class="preview" src="virl_images/previews/video_toursit_sf_preview.webp" alt="Preview image of tourist-local SF" onclick="switchVideo('Tourist', 'video1Container')">
                <img class="preview" src="virl_images/previews/video_tourist_hk_preview.webp" alt="Preview image of tourist-local HK" onclick="switchVideo('Tourist','video2Container')">
            </div>
        </div>

        <h3 id="agent-human-col">Human-Agent Collaborative Agents</h3>
        <p>
            Grounded in the same real-world as humans, agents are also capable of collaborating with humans to serve as assistants.
        </p>
        <p class="l-body">
            <img src="character_card/interactive_concierge.png" class="character-card-img">
        </p>
        <blockquote>
            As a university student in NYC, you're excited to spend a day exploring lesser-known and tranquil places. Your friend recommended Diego, who is known for his professionalism in planning practical and personalized itineraries...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
        As depicted in the following figure, Diego's itinerary is tailored to your needs. Diego not only considers your physical and mental interoception status, budget for each activity, but also anticipates your status changes and cost when you follow each event. 
        He is able to take into account <em>real</em> travel times from the <i>V-IRL</i> platform and select suitable dining options by collaborating with another 
        restaurant recommendation agent.
        </p>
        <d-figure class="l-page">
            <figure>
                <img src="virl_images/interactive_concierge.png" alt="Interactive Concierge Diego" style="width: 100%">
                <figcaption style="width: 100%"><em>The Perfect Day Itinerary</em>: Crafted by Diego, our iterative concierge agent, this schedule is meticulously tailored, accounting for your mental and physical well-being and budget variations as your day unfolds.</figcaption>
            </figure>
        </d-figure>
        <p>
            As shown in the following figure, you can intervene Diego's planning process by adjusting your interoception status or providing verbal feedback for Diego.
            In response, Diego promptly revises his original plan to make it accommodate your demands, and re-estimate your state changes after revision.
        </p>
        <d-figure>
            <figure>
                <img src="virl_images/interactive_concierge_revise.png" alt="Interactive Concierge Diego Revise" style="width: 100%">
                <figcaption style="width: 100%; text-align: center">Diego adapts original plan to suit user's intervention.</figcaption>
            </figure>
        </d-figure>

        <p>
            Additionally, grounded on tightly related street views and Map in <i>V-IRL</i>, Diego travels places in his itinerary to scout for potential scenic viewpoints for you as shown in the following figure. 
            He uses VQA to assess each captured views, attaching highly rated positions to your itinerary.
        </p>
        <!-- <d-figure>
            <figure>
                <img src="virl_images/photographer.png" alt="Interactive Concierge Diego takes photos" style="width: 130%">
                <figcaption style="width: 130%; text-align: center">Diego records attractive locations in your itinerary.</figcaption>
            </figure>
        </d-figure> -->

        <div id="slider-img-photographer" class="slider-img-container">
            <div class="my-slides">
                <!-- <div class="numbertext">1 / 5</div> -->
                <img src="virl_images/photographer/photographer_1.png" style="width:100%">
                <div class="overlay"><i>Geo Location: [40.8649162, -73.9311561]</i></div>
            </div>
          
            <div class="my-slides">
                <!-- <div class="numbertext">2 / 5</div> -->
                <img src="virl_images/photographer/photographer_2.png" style="width:100%">
                <div class="overlay"><i>Geo Location: [40.8647205, -73.9325163]</i></div>
            </div>

            <div class="my-slides">
                <img src="virl_images/photographer/photographer_3.png" style="width:100%">
                <!-- <div class="numbertext">3 / 5</div> -->
                <div class="overlay"><i>Geo Location: [40.8653388, -73.9322499]</i></div>

            </div>

            <div class="my-slides">
                <img src="virl_images/photographer/photographer_4.png" style="width:100%">
                <!-- <div class="numbertext">4 / 5</div> -->
                <div class="overlay"><i>Geo Location: [40.8609142,-73.9324818]</i></div>
            </div>

            <div class="my-slides">
                <img src="virl_images/photographer/photographer_5.png" style="width:100%">
                <!-- <div class="numbertext">5 / 5</div> -->
                <div class="overlay"><i>Geo Location: [40.8642401,-73.9325958]</i></div>
            </div>
              
            <a class="prev" onclick="plusSlides('slider-img-photographer', -1)">❮</a>
            <a class="next" onclick="plusSlides('slider-img-photographer', 1)">❯</a>
            
            <figcaption id="caption" style="margin-bottom: 10px; text-align: center">Diego rates scenery and records attractive locations in your itinerary.</figcaption>

            <div class="row">
                <div class="column">
                    <img class="demo cursor" src="virl_images/photographer/photographer_1.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-photographer', 1)" alt="Diego rates scenery and records attractive locations in your itinerary.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/photographer/photographer_2.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-photographer', 2)" alt="Diego rates scenery and records attractive locations in your itinerary.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/photographer/photographer_3.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-photographer', 3)" alt="Diego rates scenery and records attractive locations in your itinerary.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/photographer/photographer_4.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-photographer', 4)" alt="Diego rates scenery and records attractive locations in your itinerary.">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/photographer/photographer_5.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-photographer', 5)" alt="Diego rates scenery and records attractive locations in your itinerary.">
                </div>
            </div>
        </div>

        <d-figure>
            <figure>
                <img src="virl_images/interactive_concierge_pipeline.png" alt="Pipeline of Interactive Concierge agent" usemap="#imagemap" style="width: 100%">
                <figcaption style="width: 100%; text-align: center">Pipeline overview of interactive concierge agent Diego.</figcaption>
            </figure>
        </d-figure>
        

        <!-- Part 2: System fundamental -->
        <h2 id="system">System Fundamental</h2>
        <p>
            This section introduces our system's core: a platform designed for perception-driven agents that transforms real-world cities around the world into a vast virtual playground where agents can be constructed to solve practical tasks. At its heart, <i>V-IRL</i> is comprised of a hierarchical architecture. The <i>platform</i> lies at the foundation---providing the underlying components and infrastructure for agents to employ.
            Higher level <i>capabilities</i> of <em>Perception</em>, <em>Reasoning</em>, </em>Action</em>, and <em>Collaboration</em> emerge from the platform's components.
            Finally, <em>agents</em> leverage these capabilities and user-defined metadata in task-specific routines to solve tasks.
        </p>
        <p><strong>Click interested modules to see explanations:</strong></p>
        <div class="clickable-image-container">
            <img src="virl_images/architecture.png" alt="Architecture figure" style="width: 100%">

            <!-- clickable regions Agent -->
            <div class="clickable-region" style="top: 5.5%; left: 1.5%; width: 4.5%; height: 12.8%;" onclick="showInfo('agent-info');"></div>
            <div class="clickable-region" style="top: 2.8%; left: 9.5%; width: 28.5%; height: 6.8%;" onclick="showInfo('backgroundInfo');"></div>
            <div class="clickable-region" style="top: 2.8%; left: 39%; width: 28%; height: 6.8%;" onclick="showInfo('intentionInfo');"></div>
            <div class="clickable-region" style="top: 2.8%; left: 67.5%; width: 28%; height: 6.8%;" onclick="showInfo('interoceptiveInfo');"></div>
            <div class="clickable-region" style="top: 11%; left: 12%; width: 10%; height: 11%;" onclick="showInfo('agent-location');"></div>
            <div class="clickable-region" style="top: 11%; left: 27%; width: 10%; height: 11%;" onclick="showInfo('agent-biography');"></div>
            <div class="clickable-region" style="top: 11%; left: 40%; width: 10%; height: 11%;" onclick="showInfo('agent-goal');"></div>
            <div class="clickable-region" style="top: 11%; left: 55%; width: 10%; height: 11%;" onclick="showInfo('agent-task');"></div>
            <div class="clickable-region" style="top: 11%; left: 69.5%; width: 10%; height: 11%;" onclick="showInfo('agent-mental');"></div>
            <div class="clickable-region" style="top: 11%; left: 84%; width: 10%; height: 11%;" onclick="showInfo('agent-physical');"></div>
            <!-- clickable regions Capability -->
            <div class="clickable-region" style="top: 22.5%; left: 1.5%; width: 4.5%; height: 22.5%;" onclick="showInfo('capability-info');"></div>
            <div class="clickable-region" style="top: 30%; left: 10%; width: 21%; height: 7%;" onclick="showInfo('capability-perception');"></div>
            <div class="clickable-region" style="top: 30%; left: 31.5%; width: 20.5%; height: 7%;" onclick="showInfo('capability-reasoning');"></div>
            <div class="clickable-region" style="top: 30%; left: 53%; width: 20.5%; height: 7%;" onclick="showInfo('capability-action');"></div>
            <div class="clickable-region" style="top: 30%; left: 74.5%; width: 20.5%; height: 7%;" onclick="showInfo('capability-collaboration');"></div>
            <!-- clickable regions Platform -->
            <div class="clickable-region" style="top: 62.5%; left: 1.5%; width: 4.5%; height: 16.5%;" onclick="showInfo('platform-info');"></div>
            <div class="clickable-region" style="top: 46%; left: 9%; width: 47%; height: 8%;" onclick="showInfo('platform-cv');"></div>
            <div class="clickable-region" style="top: 46%; left: 57%; width: 40%; height: 8%;" onclick="showInfo('platform-lm');"></div>
            <div class="clickable-region" style="top: 73%; left: 14%; width: 77%; height: 7%;" onclick="showInfo('platform-env');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 9%; width: 13.5%; height: 15%;" onclick="showInfo('platform-cv-owr');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 23%; width: 13.5%; height: 15%;" onclick="showInfo('platform-cv-loc');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 37%; width: 13.5%; height: 15%;" onclick="showInfo('platform-cv-fm');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 51%; width: 13%; height: 15%;" onclick="showInfo('platform-cv-vqa');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 64.5%; width: 15.5%; height: 15%;" onclick="showInfo('platform-lm-tool');"></div>
            <div class="clickable-region" style="top: 55.5%; left: 80.5%; width: 15.5%; height: 15%;" onclick="showInfo('platform-lm-interact');"></div>
            <div class="clickable-region" style="top: 81%; left: 15%; width: 13.5%; height: 16%;" onclick="showInfo('platform-env-street');"></div>
            <div class="clickable-region" style="top: 81%; left: 30.5%; width: 13.5%; height: 16%;" onclick="showInfo('platform-env-geoloc');"></div>
            <div class="clickable-region" style="top: 81%; left: 45.5%; width: 13.5%; height: 16%;" onclick="showInfo('platform-env-move');"></div>
            <div class="clickable-region" style="top: 81%; left: 61%; width: 13.5%; height: 16%;" onclick="showInfo('platform-env-map');"></div>
            <div class="clickable-region" style="top: 81%; left: 76.5%; width: 13.5%; height: 16%;" onclick="showInfo('platform-env-place');"></div>
        </div>
        
        <!-- aside information -->
        <!-- aside information Agent -->
        <aside class="system-fundamental-aside" id="agent-info">
            <p>In our system, <strong>agent behavior</strong> is shaped by user-defined metadata, including a background, an intended goal, and an interoceptive state.</p>
            <p>Concretely, agents are developed by writing task-specific <code>run()</code> routines that leverage the various components of our platform and the agent's metadata to solve tasks.</p>
        </aside>
        <aside class="system-fundamental-aside" id="backgroundInfo">
            The <strong>background</strong> provides the context necessary to instantiate the agent in the real world (location), and to guide its reasoning and decision making (biography).
        </aside>
        <aside class="system-fundamental-aside" id="intentionInfo"> <strong>Intentions</strong> outline agents' purpose within the environment.</aside>
        <aside class="system-fundamental-aside" id="interoceptiveInfo">An agent's <strong>interoceptive state</strong> reflects its internal mental and physical status -- varying over time and influencing its behavior.</aside>
        <aside class="system-fundamental-aside" id="agent-location">
            <p>User-defined agent's initial <strong>location</strong>, can be geolocation or address.</p> 
            <p>All exampling <i>V-IRL</i> agents leverages on this information.</p></aside>
        <aside class="system-fundamental-aside" id="agent-biography">
            <p>User-defined agent's <strong>biography</strong>, can include any information (e.g. age, job, hometown, personality, relationship, etc) about the agent.</p>
            <p>This information typically helps agent make decisions, such as Place Recommender agents Aria and Vivek, Intentional Explorer agent Hiro and Interactive Concierge agent Diego.</p>
        </aside>
        <aside class="system-fundamental-aside" id="agent-goal">
            <p>User-defined agent's <strong>goal</strong> (intention), specifying the current goal of the agent as the "intention" term in each <i>character card</i>.</p>
            <p>This information typically helps agent make decisions, such as Place Recommender agents Aria and Vivek, Intentional Explorer agent Hiro and Interactive Concierge agent Diego.</p>
        </aside>
        <aside class="system-fundamental-aside" id="agent-task">
            <p>Agent's <strong>task</strong> describes its high-level behavior as the "Task" term in each <i>character card</i>.</p>
            <p>This information will not directly influence in any concrete agent behavior, but serves as a overall definition to agent's behavior</p>
        </aside>
        <aside class="system-fundamental-aside" id="agent-mental">
            <p> User-defined agent's <strong>mental state</strong>, can be custom defined to any aspects such as joy, stress, sadness, etc.</p>
            <p> This information is mainly used in the "Interoceptive Estimator" and "Supervisor" of Interactive Concierge agent Diego.</p>
        </aside>
        <aside class="system-fundamental-aside" id="agent-physical">
            <p>User-defined agent's <strong>physical state</strong>, can be custom defined to any aspects such as hunger, energy, pain, etc.</p>
            <p> This information is mainly used in the "Interoceptive Estimator" and "Supervisor" of Interactive Concierge agent Diego.</p>
        </aside>
        <!-- aside information Capability -->
        <aside class="system-fundamental-aside" id="capability-info">
            <p><strong>Capability</strong> bridges agent and platform. Agents are able to leverage different capabilities along with user-defined metadata to solve tasks.</p>
            <p>Our platform's components can be flexibly combined to accomplish a multitude of tasks.
            In <a href="#agent-examplers"><i>V-IRL agents section</i></a>, we present agents that exhibit increasingly complex behaviors, each requiring more components of the platform.
            From simple combinations, like the Route Optimizer agent Peng, to more complex arrangements, 
            like the Tourist, our system showcases the versatility and potential of the <i>V-IRL</i> framework to be applied to a wide range of real-world scenarios.</p>
        </aside>
        <aside class="system-fundamental-aside" id="capability-perception"> 
            <p><strong>Perception</strong> capability enables agents to process the sensory-rich data provided by the <i>environment</i>, especially street view imagery.</p>
            <p>This capability is used in agents with <img src="virl_images/tags/cv.png" class="inline-tag">: RX-399, Urban Planner, Intentional Explorer, Tourist and Interactive Concierge.</p>
        </aside>
        <aside class="system-fundamental-aside" id="capability-reasoning">
            <p><strong>Reasoning</strong> capability allows decision making based on information from perception and the environment.</p>
            <p>This capability is used in agents with <img src="virl_images/tags/lm.png" class="inline-tag">: Place Recommender, Intentional Explorer, Tourist, Local and Interactive Concierge.</p>
        </aside>
        <aside class="system-fundamental-aside" id="capability-action">
            <p><strong>Action</strong> capability is responsible for grounding agents in the world around them: providing a navigable representation and geospatial information of real cities.</p>
            <p>This capability is used in agents with <img src="virl_images/tags/geo.png" class="inline-tag"> (all agents).</p>
        </aside>
        <aside class="system-fundamental-aside" id="capability-collaboration">
            <p><strong>Collaboration</strong> capability enables the interact between agents or with humans.</p>
            <p>This capability is used in agents with <img src="virl_images/tags/col.png" class="inline-tag">, such as Tourist-Local and Interactive Concierge.</p>
        </aside>
        <!-- aside information Platform -->
        <aside class="system-fundamental-aside" id="platform-info">
            <p><strong>Platform</strong> components, which provide the infrastructure to instantiate capabilities, execute agent actions, and ground agents in the real world. </p>
        </aside>
        <aside class="system-fundamental-aside" id="platform-cv">
            <p><strong>Computer Vision</strong> modules instantiate concrete vision models used in <i>perception</i> capability
        </aside>
        <aside class="system-fundamental-aside" id="platform-lm">
            <strong>Language Model</strong> modules instantiate LLMs which allow question answering, interaction and tool & API use.</aside>
        <aside class="system-fundamental-aside" id="platform-env">
            <strong>Environment</strong> modules are responsible for grounding agents in the world around them: providing a navigable representation of real cities.
        </aside>
        <aside class="system-fundamental-aside" id="platform-cv-owr">
            <strong>Open-world recognition</strong> models <d-cite key="radford2021learning"></d-cite> are more general than localization models, and allow agents to detect a wider range of objects in their field of view (e.g., Tourist searches for the Apple Store).
        </aside>
        <aside class="system-fundamental-aside" id="platform-cv-loc">
            <strong>Localization</strong> models <d-cite key="li2022grounded"></d-cite> give agents a precise spatial understanding of their environment.
            This allows RX-399 to identify and count instances of objects, and Hiro to pick out specific businesses to look up with the environment.
        </aside>
        <aside class="system-fundamental-aside" id="platform-cv-fm">
            <strong>Feature matching</strong> models <d-cite key="lindenberger2023lightglue"></d-cite> provide an understanding of continuity across views of the same location, and enable agents to identify & deduplicate instances of the same object from different viewpoints in RX-399, Urban Planner and Intentional Explorer.
        </aside>
        <aside class="system-fundamental-aside" id="platform-cv-vqa">
            Multimodal models with <strong>VQA</strong> capabilities <d-cite key="li2023blip"></d-cite> bridge the perceptual world with natural language, and
            are essential for integration with <i>reasoning</i>. This allows agents to verbalize visual information to natural language in a more open-end aspect, such as Intentional Explorer Hiro asks "What is road is better?" in the intersections.
        </aside>
        <aside class="system-fundamental-aside" id="platform-lm-tool">
            LLMs such as GPT-4 <d-cite key="openai2023gpt"></d-cite> and Llama~2 <d-cite key="touvron2023llama"></d-cite>
            interface across various <strong>APIs</strong> (see <a href="#agent-language">language-driven agents examples</a>), transforming environmental data and perceptual outputs into actionable insights.
        </aside>
        <aside class="system-fundamental-aside" id="platform-lm-interact">
            LLMs also enable collaboration between agents or with humans through natural language (see <a href="#agent-agent-col">collaborative agents</a>). Custom prompts facilitate this interaction. 
        </aside>
        <aside class="system-fundamental-aside" id="platform-env-street">Agents can extract <strong>street view imagery</strong> by specifying its geolocation, fov and ego-pose. Besides, this module provides a one-to-one correlation between street view panorama and geolocation.</aside>
        <aside class="system-fundamental-aside" id="platform-env-geoloc">Agents can translate natural language address to <strong>geolocation</strong>, and get its current geolocation.</aside>
        <aside class="system-fundamental-aside" id="platform-env-move">Through the <strong>movement</strong> module, agent can obtain all navigable directions and position with street view imagery around its current location.</aside>
        <aside class="system-fundamental-aside" id="platform-env-map"><strong>Mapping</strong> module allows agents to get available routing, time and distance from a position to another.</aside>
        <aside class="system-fundamental-aside" id="platform-env-place"><strong>Place Info & Search</strong> module enables nearby place searches with hyper-parameters such as name, place type, radius, etc. Also, it provides place information such as place address, reviews, photos and so on.</aside>


        <figcaption style="width: 100%; text-align: center">Hierarchical <i>V-IRL</i> architecture.</figcaption>


        <!-- Part 3: Benchmark -->
        <h2 id="virlbenchmark"><em>V-IRL</em> Benchmark</h2>
        <p>
            The essential attributes of <i>V-IRL</i> include its ability to access geographically diverse data derived from real-world sensory input, and its API that facilitates interaction with Google Map Platform (GMP) <d-cite key="google_map_platform"></d-cite>. 
            This enables us to develop three <i>V-IRL</i> benchmarks to assess the capabilities of existing vision models in such open-world data distribution.
        </p>

        <h3><em>V-IRL Place:</em> Localization</h3>
        <p>
            <strong>Motivation:</strong> Every day, humans traverse through cities, moving between diverse places to fulfil a range of goals, like the Intentional Explorer agent.
            We assess the performance of vision models on the everyday human activity <i>place localization</i> using street view imagery and associated place data.
        </p>
        <p>
            <strong>Setups:</strong> We modify RX-399 agent to traverse polygonal areas while localizing & identifying 20 types of places.
            We evaluate three prominent open-world detection models:  GroundingDINO <d-cite key="liu2023grounding"></d-cite>, GLIP <d-cite key="li2022grounded"></d-cite> and Owl-ViT <d-cite key="minderer2022simple"></d-cite>. We also implement a straightforward baseline, CLIP (w/ GLIP proposal), which involves reclassifying the categories
            of GLIP proposals with CLIP <d-cite key="radford2021learning"></d-cite>.
            Models are evaluated on localization recall, which is quantified as 
            <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mfrac>
                  <mrow>
                    <mi>N</mi>
                    <msub>
                      <mi></mi>
                      <mi>tp</mi>
                    </msub>
                  </mrow>
                  <mrow>
                    <mi>N</mi>
                    <msub>
                      <mi></mi>
                      <mi>tp</mi>
                    </msub>
                    <mo>+</mo>
                    <mi>N</mi>
                    <msub>
                      <mi></mi>
                      <mi>fn</mi>
                    </msub>
                  </mrow>
                </mfrac>
              </math>, where 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>N</mi>
                <msub>
                  <mi></mi>
                  <mi>tp</mi>
                </msub>
              </math> 
              and 
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mi>N</mi>
                <msub>
                  <mi></mi>
                  <mi>fn</mi>
                </msub>
              </math>
            represents the number of correctly localized places and missed places, respectively.
        </p>

        <d-figure>
            <figure style="margin-top: 5px; margin-bottom: 15px">
                <img src="virl_images/bm_local_matching.png" alt="V-IRL Place Localization" style="width: 80%;display: block; margin: auto;">
                <figcaption style="width: 100%;">Matching between 2D object proposal and street place. we first <i>project</i> the bounding box of each object proposal onto a frustum in the 3D space, subject to a radius.
                We then determine if any <i>nearby places</i> fall within this frustum and radius. 
                If any nearby place is found, the closest one is assigned as the <i>ground truth</i> for the object proposal. Otherwise, the object proposal is regarded as a <i>false positive</i>. 
                When multiple places are inside the frustum, we consider the nearest one as the ground truth since it would likely block the others in the image.</figcaption>
            </figure>
        </d-figure>

        <p>
            <strong>Results:</strong> Following table shows that open-world detectors like GroundingDINO <d-cite key="liu2023grounding"></d-cite>, Owl-ViT <d-cite key="minderer2022simple"></d-cite> and GLIP <d-cite key="li2022grounded"></d-cite> are biased towards certain place types such as <code>school</code>, <code>cafe</code>, and <code>convenience store</code>, respectively. 
            In contrast, CLIP (w/ GLIP proposal) can identify a broader spectrum of place types.
            This is mainly caused by the category bias in object detection datasets with a limited vocabulary. 
            Hence, even if detectors like Owl-ViT are initialized with CLIP, their vocabulary space narrows down due to fine-tuning. 
            These results suggest that cascading category-agnostic object proposals to zero-shot recognizers appears promising for "real" open-world localization, especially for less common categories in object detection datasets.
        </p>

        <d-figure>
            <figure style="margin-bottom: 10px; margin-top: 0px ">
                <img src="virl_images/bm_table_localization.png" alt="V-IRL Place Localization Results" style="width: 100%">
                <figcaption style="width: 100%;">Benchmark results on <i>V-IRL</i> Place Localization. <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>AR</mi>
                    <msup>
                      <mrow></mrow>
                      <mn>10</mn>
                    </msup>
                  </math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mi>AR</mi>
                    <msup>
                      <mrow></mrow>
                      <mn>20</mn>
                    </msup>
                  </math> denote average recall on subsampled 10 and all 20 place categories, respectively. More results in paper.</figcaption>
            </figure>
        </d-figure>

        <h3><em>V-IRL Place:</em> Recognition and VQA</h3>
        <p>
            <strong>Motivation:</strong> In contrast to the challenging <i>V-IRL</i> place localization task on street view imagery, in real life, humans can recognize businesses by taking a closer, <strong>place-centric</strong> look.
            In this regard, we assess existing vision models on two perception tasks based on place-centric images: <i>i)</i> recognizing specific place types; <i>ii)</i> identifying human intentions by Vision Question Answering (VQA), named intention VQA.
        </p>

        <p style="margin-bottom: 5px;">
            <strong>Setups:</strong> For recognition, we assess 10 open-world recognition models, for place type recognition from 96 options, using <i>place-centric images</i> (see below imagery illustration).
        </p>
        <div id="slider-img-street-place" class="slider-img-container">
            <div class="my-slides">
                <img src="virl_images/streetview_vs_place/streetview_vs_placeimage_1.png" style="width:100%">
            </div>
            
            <div class="my-slides">
                <img src="virl_images/streetview_vs_place/streetview_vs_placecentric_2.png" style="width:100%">
            </div>

            <div class="my-slides">
                <img src="virl_images/streetview_vs_place/streetview_vs_placecentric_3.png" style="width:100%">    
            </div>
                
            <a class="prev" onclick="plusSlides('slider-img-street-place', -1)">❮</a>
            <a class="next" onclick="plusSlides('slider-img-street-place', 1)">❯</a>
            
            <figcaption style="margin-bottom: 10px;">Street view imagery (left), sourced from the Google Street View database, are taken from a street-level perspective, encompassing a broad view of the surroundings, including multiple buildings. 
            Place-centric imagery (right), drawn from the Google Place database, focus predominantly on the specific place, providing a more concentrated view.</figcaption>

            <div class="row">
                <div class="column">
                    <img class="demo cursor" src="virl_images/streetview_vs_place/streetview_vs_placeimage_1.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-street-place', 1)" alt="Street view imagery (left) vs place-centric imagery (right).">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/streetview_vs_place/streetview_vs_placecentric_2.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-street-place', 2)" alt="Street view imagery (left) vs place-centric imagery (right).">
                </div>
                <div class="column">
                    <img class="demo cursor" src="virl_images/streetview_vs_place/streetview_vs_placecentric_3.png" style="height: 60px; width: auto" onclick="currentSlide('slider-img-street-place', 3)" alt="Street view imagery (left) vs place-centric imagery (right).">
                </div>
            </div>
        </div>
        <p>
        For intention VQA, we also evaluate 8 multi-modal large language models (MM-LLM) to determine viable human intentions from a four-option multiple-choice VQA. The <i>V-IRL Place</i> VQA process is illustrated in following image, where the candidate and true choices are generated by GPT-4 <d-cite key="openai2023gpt"></d-cite> given the place types and place names corresponding to the image.
        <figure style="margin-top: 0px; margin-bottom: 0px">
            <img src="virl_images/bm_vqa_example.png" alt="V-IRL Place VQA" style="width: 80%; display: block; margin: auto;">
            <figcaption style="width: 100%; text-align: center">Example of <i>V-IRL Place</i> VQA process.</figcaption>
        </figure>
        </p>

        <p>
            <strong>Results:</strong> Following table shows that CLIP (L/14@336px) outperforms even the biggest version of Eva-02-CLIP and SigLIP in the <i>V-IRL Place</i> recognition task, emphasizing the high-quality data of CLIP. 
            The bottom of the table shows that in the intention VQA, 
            BLIP2 <d-cite key="li2023blip"></d-cite>, InstructBLIP <d-cite key="dai2023instructblip"></d-cite> and LLaVA-1.5 <d-cite key="liu2023improvedllava"></d-cite> excel, whereas others struggle. 
            We note that these three top-performing MM-LLMs provide consistent answers in the circular evaluation, while others frequently fail due to inconsistent choice selection.
        </p>
        <figure style="margin-top: 0px; margin-bottom: 0px">
            <img src="virl_images/bm_table_rec_vqa.png" alt="V-IRL Place VQA" style="width: 80%; display: block; margin: auto;">
            <figcaption style="width: 100%; text-align: left">Benchmark results on <i>V-IRL Place</i> recognition and <i>V-IRL Place</i> VQA. <span class="resscolor">Green</span> indicates increased resolution models, while <span class="sizescolor">Blue</span> denotes model parameter scaling.</figcaption>
        </figure>

        <h3><em>V-IRL</em> Vision Language Navigation</h3>
        <p>
            <strong>Motivation:</strong> As discussed in the <i>V-IRL</i> agents section, Intentional Explorer and Tourist agents require collaboration between vision models and language models to accomplish complex tasks. Therefore, this motivates us to investigate the performance of vision-language collaboration, with environmental information acquired through visual perception models from real-world images. 
            This prompts us to build an embodied task for jointly leveraging vision and language models along with the realistic street views in <i>V-IRL</i>. 
            In this regard, we build this <i>V-IRL</i> Vision Language Navigation (VLN) benchmark.
        </p>

        <p>
            <strong>Setups:</strong> We adapt the Tourist agent implementation and replace its recognition component with the various benchmarked models. These methods are tasked to identify visual landmarks during navigation. Subsequently, GPT-4 <d-cite key="openai2023gpt"></d-cite> predicts the next action according to the recognition results. Navigation instructions are generated using the Local agent.<br> 
            Four approaches are evaluated to recognize landmarks during navigation: <i>(i)</i> Approximate oracle by searching nearby landmarks; <i>(ii)</i> Zero-shot recognizers CLIP <d-cite key="radford2021learning"></d-cite> and EVA-02-CLIP <d-cite>EVA-CLIP</d-cite>; <i>(iii)</i> Multi-modal LLM LLaVA-1.5 <d-cite key="liu2023improvedllava"></d-cite>
            <i>(iv)</i> OCR model <d-cite key="du2009pp"></d-cite> to recognize potential text in street views followed by GPT answer parsing.
        </p>

        <p>
            <strong>Results:</strong> Following table shows that, with <em>oracle landmark information</em>, powerful LLMs can impressively comprehend navigation instructions and thus make accurate decisions. However, when using vision models to fetch landmark information from street views, the success rate drops dramatically, suggesting that the perception of vision models is noisy and misguides LLMs' decision making. Among these recognizers, larger variants of CLIP <d-cite key="radford2021learning"></d-cite> and EVA-02-CLIP <d-cite key="EVA-CLIP"></d-cite> perform better, highlighting the benefits of model scaling. LLaVA-1.5 <d-cite key="liu2023improvedllava"></d-cite> shows inferior performance with CLIP (L/14@336px) as its vision encoder, possibly due to the alignment tax <d-cite key="openai2023gpt"></d-cite> during instruction tuning. 
            Further, PP-OCR <d-cite key="du2009pp"></d-cite> (+ GPT-3.5) achieves a 28% success rate, signifying that OCR is crucial for visual landmark recognition. 
        </p>

        <d-figure>
            <figure style="margin-top: 0px; margin-bottom: 0px">
                <img src="virl_images/bm_table_vln.png" alt="V-IRL VLN" style="width: 80%; display: block; margin: auto;">
                <figcaption style="width: 100%; text-align: left">Results on <i>V-IRL</i> VLN-mini. We test various CLIP-based models, <span class="resscolor">MM-LLM</span>, and <span class="sizescolor">OCR</span> model with GPT postprocessing.  We primarily measure navigation success rate (<i>Success</i>). In addition, as navigation success is mainly influenced by the agent's actions at key positions (<i>i.e.</i>, start positions, intersections and stop positions), we also evaluate the arrival ratio (<i>Arr</i>) and reaction accuracy (<i>Reac</i>) for each route. Arr denotes the percentage of key positions reached, while Reac measures the accuracy of the agent's action predictions at these key positions. Full-set results on CLIP and Oracle are available in paper appendix.</figcaption>
            </figure>
        </d-figure>

        <h3>Geographic Diversity</h3>
        <p>
            Spanning 12 cities across the globe, our <i>V-IRL</i> benchmarks provide an opportunity to analyze the inherent model biases in different regions. As depicted in the following figure, vision models demonstrate subpar performance on all three benchmark tasks in Lagos, Tokyo, Hong Kong, and Buenos Aires.
            In Lagos, vision models might struggle due to its non-traditional street views relative to more developed cities (see street views in aside figures). For cities like Tokyo, Hong Kong and Buenos Aires, an intriguing observation is their primary use of non-English languages in street views.
            This suggests that existing vision models face challenges with multilingual image data.
        </p>
        <aside class="img-magnifier-container">
            <img id="street_lagos" src="virl_images/bm_city/street_lagos.png" style="width: 240px; height: auto">
            <figcaption style="width: 240px; text-align: center">Lagos, Nigeria.</figcaption>
        </aside>

        <div class="img-magnifier-container">
            <img id="bm_city_img" src="virl_images/bm_city_analysis.png" alt="City level analysis" style="width: 100%">
            <figcaption style="width: 100%; text-align: center">City-level visualization of <i>V-IRL</i> benchmark results.</figcaption>
        </div>
        <aside>
            <div class="img-magnifier-container">
                <img id="street_tokyo" src="virl_images/bm_city/street_tokyo.png" style="width: 240px; height: auto">
                <figcaption style="width: 240px; text-align: center; margin-bottom:10px">Tokyo, Japan </figcaption>
            </div>
            <div class="img-magnifier-container">
                <img id="street_hk" src="virl_images/bm_city/street_hk.png" style="width: 240px; height: auto">
                <figcaption style="width: 240px; text-align: center; margin-bottom:10px">Hong Kong, China.</figcaption>
            </div>
            <div class="img-magnifier-container">
                <img id="street_ba" src="virl_images/bm_city/street_Buenos_Aires.png" style="width: 240px; height: auto">
                <figcaption style="width: 240px; text-align: center; margin-bottom:10px">Buenos Aires, Argentina.</figcaption>
            </div>
        </aside>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{VIRL,<br>
                &nbsp;&nbsp;title={V-IRL: Grounding Virtual Intelligence in Real Life},<br>
                &nbsp;&nbsp;author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},<br>
                &nbsp;&nbsp;year={2023},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2312},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          </d-appendix>
          
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

          <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <!-- <script type="text/bibliography">
        
        </script> -->
        <script src="contents_bar.js"></script>
        
        <script type="text/javascript">
            magnify("urban_planner_img", 2);
            magnify("bm_city_img", 1.5);
            // magnify("street_lagos", 2, 80, 80);
            // magnify("street_tokyo", 2, 80, 80);
            // magnify("street_hk", 2, 80, 80);
            // magnify("street_ba", 2, 80, 80);

            let slideIndex = 1;
            showSlides("slider-img-rx399", slideIndex);
            showSlides("slider-img-tourist", slideIndex);
            showSlides("slider-img-photographer", slideIndex);
            showSlides("slider-img-street-place", slideIndex);
   
        </script>
    </body>
</html>
