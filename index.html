<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        <script src="image_interact.js"></script>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1><em>V-ILR</em>: Grounding Virtual Intelligence in Real Life</h1>
              <div class="button-container">
                <a href="#" class="button">Paper</a>
                <a href="#" class="button">Code</a>
              </div>
            </div>
            <div class="header-video">
                <video autoplay loop muted>
                    <source src="videos/teaser_video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="https://jihanyang.github.io/" class="author-link">Jihan Yang</a></p>
                    <p><a href="https://dingry.github.io/" class="author-link">Runyu Ding</a></p>
                    <p><a href="https://ellisbrown.github.io/" class="author-link">Ellis Brown</a></p>
                    <p><a href="https://xjqi.github.io/" class="author-link">Xiaojuan Qi</a></p>
                    <p><a href="https://www.sainingxie.com/" class="author-link">Saining Xie</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                    <p><a href="https://www.eee.hku.hk/" class="affiliation-link">University of Hong Kong</a></p>
                    <p><a href="https://cs.nyu.edu/home/index.html" class="affiliation-link">New York University</a></p>
                </div>
                <div class="byline-column">
                    <h3>Date</h3>
                    <p>Dec. x, 2023</p>
                </div>
            </div>
        </div>

        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#agent-examplers">Virtual Intelligence in Real Life</a></div>
                <ul>
                    <li><a href="#agent-earthbound" style="font-size: 12px;">Earthbound Agents</a></li>
                    <li><a href="#agent-language" style="font-size: 12px;">Language-Driven Agents</a></li>
                    <li><a href="#agent-vision" style="font-size: 12px;">Visually Grounded Agents</a></li>
                    <li><a href="#agent-agent-col" style="font-size: 12px;">Agent-Agent Collaborative Agents</a></li>
                    <li><a href="#agent-human-col" style="font-size: 12px;">Human-Agent Collaborative Agents</a></li>
                </ul>
                <div><a href="#system">System Fundamental</a></div>
                <div><a href="#virlbenchmark"><em>V-IRL</em> Benchmark</a></div>
            </nav>
        </d-contents>
        
        <p>There is a sensory gulf between the Earth that humans inhabit and the imaginary, digital realms in which modern AI agents are created.
        To develop AI agents that can sense, think, and act as flexibly as humans in real-world settings, it's essential to bridge the gap between the digital and physical worlds.
        How can we embody agents in an environment as rich and diverse as the one we inhabit, without the constraints imposed by real hardware and control?
        Towards this end, we introduce <strong><i>V-IRL</i></strong>: a platform that enables agents to scalably interact with the real-world in a virtual yet realistic environment.
        Our platorm serves as a playground for developing agents that can accomplish a variety of practical tasks, and as a vast testbed for measuring progress in capabilities spanning perception, decision-making, and interaction with real-world data spanning the entire globe.
        </p>

        <h2 id="agent-examplers">Virtual Intelligence in Real Life</h2>
        <p>
            To show the versatility of the <strong><i>V-IRL</i></strong> platform, we use it to instantiate several exemplar agents in our virtual real-world environment. For illustration, we give <strong><i>V-IRL</i></strong> agents character metadata, including an 8-bit avatar, a name, a short bio, and an intention they are trying to accomplish.
        </p>

        <h3 id="agent-earthbound">Earthbound Agents</h3>
        <p>
        Agents in the <i>V-IRL</i> platform inhabit virtual representations of real cities around the globe. At the core of this representation are <em>geographic coordinates</em> corresponding to points on the Earth's surface. Using these coordinates, <i>V-IRL</i> allows agents to <i>ground</i> themselves in the world using maps, <em>real</em> street view imagery, information about nearby destinations, and additional data from arbitrary geospatial APIs
        </p>
        <p class="l-body">
            <img src="character_card/courier.png" class="character-card-img">
        </p>
        <blockquote> 
            Peng needs to visit several locations throughout the city to get documents signed for registration as a visiting student...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Leveraging <u class="geoul">Geolocation & Mapping</u> capabilities, Peng saves 7 minutes by 
            walking along the shortest path as opposed to in order waypoint visitation.
            <d-figure>
                <figure>
                    <img src="virl_images/courier.png" alt="Route optimizer figure">
                    <figcaption>Finding the shortest path for Peng to travel to five places.</figcaption>
                </figure>
            </d-figure>
        </p>
        

        <h3 id="agent-language">Language-Driven Agents</h3>
        <p>
            To tackle more complex tasks, we follow the pattern of language-driven agents <d-cite key="rise"></d-cite>. LLMs
            enable agents to reason, plan  
            and use external tools & APIs.
        </p>
        <p class="l-body">
            <img src="character_card/place_recommender.png" class="character-card-img">
        </p>
        <!-- place recommender agent -->
        <blockquote> 
            Peng is starving for some lunch but doesn't know where to eat...<br>
            Luckily, he met a nice grad student Aria during his errands who might be able to help him find a good spot...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Aria <u class="geoul">searches</u> for possible restaurants nearby.
            She then synthesizes public reviews to make final recommendations via <u class="llmul">GPT-4</u>. As Peng is new to the city and originally from Sichuan, she recommends the spicy Chinese joint <em>Chow House 粤德轩</em> to give him a taste of home.
        </p>
        <div class="interactive-image">
            <div class="button-bar">
                <!-- Buttons to interact with the image and text -->
                <button onclick="changeContent('Place1')">Chow House</button>
                <button onclick="changeContent('Place2')">Kwa Food Deep Fried Skewers</button>
                <button onclick="changeContent('Place3')">Tartinery Cafe</button><br>
                <button style="margin-left: 20%" onclick="changeContent('Place4')">Sushi Zo</button>
                <button style="margin-left: 15%" onclick="changeContent('Place5')">Dos Toros Taqueria</button>
            </div>

            <div id="image-text-container">
            <d-figure>
                <figure>
                    <img src="virl_images/place_recommend/place1.png" alt="place illustration">
                    <figcaption>Chow House is a highly recommended Sichuan restaurant, which aligns with Peng's background as he grew up in Sichuan. The restaurant offers authentic Sichuan food, which Peng might be familiar with and enjoy. The restaurant also has good seating, decoration, and friendly service, which would make for a pleasant dining experience. However, some dishes received mixed reviews, which is why the rating is not a perfect 10.</figcaption>
                </figure>
            </d-figure>
            <!-- <div id="display-image">
                <img src="virl_images/place_recommend/place1.png" alt="Place 1" id="image">
            </div>
            <div id="display-text">Chow House is a highly recommended Sichuan restaurant, which aligns with Peng's background as he grew up in Sichuan. The restaurant offers authentic Sichuan food, which Peng might be familiar with and enjoy. The restaurant also has good seating, decoration, and friendly service, which would make for a pleasant dining experience. However, some dishes received mixed reviews, which is why the rating is not a perfect 10.</div> -->
            </div>
        </div>
        
        <!-- Real estate agent -->
        <blockquote> 
            Peng hires Vivek to help him find an apartment in East Village, Jersey City, or Long Island City for $1k--$3k per month close to a gym, supermarket, and public transit...
        </blockquote> 
        <hr class="l-body fancybreak">
        <p>
            Vivek uses real estate <u class="llmul">APIs</u> to find potential apartments in Peng's desired regions and price range.
            For each candidate, he researches its <u class="geoul">proximity</u> to the places Peng cares about. Synthesizing these factors, Vivek provides a holistic rating and accompanying reasoning using <u class="llmul">GPT-4</u>.
            His top recommendation is a cost-effective 1 bedroom apartment for $1986/mo, which is close to a supermarket, 2 bus stations, and a gym.
        </p>
        
        <!-- Vision agents -->
        <h3 id="agent-vision">Visually Grounded Agents</h3>
        <p>
            Although language-driven agents can address some real-world tasks using external tools, their reliance on solely text-based information limits their applicability to tasks where <em>visual grounding</em> is required.
            In contrast, <em>real sensory input</em> is integral to many daily human activities---allowing a deep connection to and understanding of the 
            real world around us.
            Agents can leverage street view imagery through the <i>V-IRL</i> platform to <em>visually ground</em> themselves in the real world---opening up a wide range of <em>perception-driven tasks</em>.
        </p>

        <!-- RX-399 -->
        <p class="l-body">
            <img src="character_card/rx399.png" class="character-card-img">
        </p>
        <blockquote> 
            RX-399 is a state-of-the-art robot agent with advanced navigation and sensing capabilities. Its manufacturer is running a pilot program with sanitation departments in Hong Kong and New York City to assess its readiness for garbage duty...
        </blockquote>
        <hr class="l-body fancybreak">

        <p>
            RX-399 navigates along pre-defined city routes, tagging all trash bins using its <u class="cvul" >open-world detector</u> and <u class="geoul">geolocation</u> module as depicted in the following figure and videos. 
            <d-figure>
                <figure>
                    <img src="virl_images/rx-399_clean.png" alt="RX-399 detects trash bin illustration">
                    <figcaption style="text-align: center">Portions of RX-399's system records in Hong Kong and New York City.</figcaption>
                </figure>
            </d-figure>
        </p>

        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Q6Hty1BTLdw?si=XAK_3qkchKgrDVit" title="RX-399 in New York City" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/9gudvNT5Ces?si=j4K_7ULDvNMIt5dp" title="RX-399 in Hong Kong" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        
        <!-- Urban Planner -->
        <p class="l-body">
            <img src="character_card/urban_planner.png" class="character-card-img">
        </p>
        <blockquote>
            Imani needs to analyze the distribution of trash bins, fire hydrants, and park benches in New York's Central Park for a project with the NYC Parks & Recreation department...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            Imani sets <u class="geoul">routes</u> spanning Central Park and objects of interest for RX-399, who traverses the routes and records all <u class="cvul">detected instances</u>. 
            After RX-399 finishes its route, Imani analyzes the collected data by RX-399 at different levels of detail.
            <!-- As depicted in \cref{fig:agent_urban_plan}, the coarsest level shows general \geoul{distributions} of trash bins, hydrants, and benches in the park. -->
            <!-- Imani can also zoom in to specific regions, where lighter colors represent positions with more unique instances identified.  -->
        </p>
        <d-figure class="l-page">
            <figure>
                <img src="virl_images/urban_planner.png" alt="Urban Planner agent visualization">
                <figcaption>Imani's visualization of trash bins, fire hydrants, park benches in NYC's Central Park using data collected by RX-399. The coarsest level shows general <u class="geoul">distributions</u> of trash bins, hydrants, and benches in the park.
                Imani can also zoom in to specific regions, where lighter colors represent positions with more unique instances identified.</figcaption>
            </figure>
        </d-figure>
        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Tq4fkKXrxYo?si=663Xhkzoq6XOONgF" title="Urban Planner agent video illustration" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/Tj_3A1vHUZU?si=d78eIoIIBZjPbvEB" title="Play on heatmap" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>

        <!-- Intentional explorer -->
        <p class="l-body">
            <img src="character_card/intentional_explorer.png" class="character-card-img">
        </p>
        <blockquote>
            Hiro is starting a new journey in Hong Kong.
            He decides to explore without a specific destination in mind, looking for a good local lunch spot with food that's not too spicy...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            As depicted in the following figure and video, driven by his intention, Hiro uses <u class="cvul">VQA</u> to select proper roads; uses <u class="cvul">open-world detection</u> to find a restaurant; uses <u class="geoul">place reviews</u> and <u class="llmul">LLM</u> to decide whether a place is suitable for his purpose.
        </p>

        <d-figure>
            <figure>
                <img src="virl_images/intentional_explorer.png" style="width: 130%" alt="Intentional Explorer agent exploring">
                <figcaption style="width: 130%; text-align: center">Visualization for Hiro's lunch exploration in HK. Concrete procedure is depicted in the following video.</figcaption>
            </figure>
        </d-figure>

        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/bOGa7eI-K_I?si=MPPoxbxf8S7Dj7Tp" title="Intentional Explorer agent exploration procedure" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        
        <h3 id="agent-agent-col">Agent-Agent Collaborative Agents</h3>
        <p>
            As with previous agents, <u class="colul">collaborative</u> agents are designed for specific tasks; however, they can handle objectives beyond their expertise through collaboration with each other.
        </p>
        
        <p class="l-body">
            <img src="character_card/tourist.png" class="character-card-img">
        </p>
        <blockquote>
            Ling travels to cities around the world. She 
            seeks out authentic experiences and
            is always unafraid to ask for help from Locals whenever she finds herself lost...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
            After obtaining route descriptions from Locals, Ling starts her journey. Grounded in our embodied platform, Ling can adjust her pose and identify visual landmarks along the streets using <u class="cvul">open-world recognition</u> and her <u class="geoul">map</u>. Recognizing these landmarks helps <u class="llmul">GPT-4</u> to make correct decisions about where to turn direction, move forward and stop. Concrete examples are shown in the following figure and videos.
        </p>
        <d-figure>
            <figure>
                <img src="virl_images/navigation.png" alt="Tourist-Local collaboration" style="width: 130%">
                <figcaption style="width: 130%; text-align: center">Ling and Local collaboration examples. Trajectories in red and green mean Ling's first and second attempts, respectively.</figcaption>
            </figure>
        </d-figure>

        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/kS2zIDyVvSU?si=hoA4oI4sKXWDk1Wy" title="Tourist-Local collaboration in Hong Kong" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>
        <p class="video-container">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/H_F-_NoHDLk?si=7TcYGkiFOIBaCHZo" title="Tourist-Local collaboration in San Francisco" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </p>

        <h3 id="agent-human-col">Human-Agent Collaborative Agents</h3>
        <p>
            Grounded in the same real-world as humans, agents are also capable of collaborating with humans to serve as assistants.
        </p>
        <p class="l-body">
            <img src="character_card/interactive_concierge.png" class="character-card-img">
        </p>
        <blockquote>
            As a university student in NYC, you're excited to spend a day exploring lesser-known and tranquil places. Your friend recommended Diego, who is known for his professionalism in planning practical and personalized itineraries...
        </blockquote>
        <hr class="l-body fancybreak">
        <p>
        As depicted in the following figure, Diego's itinerary is tailored to your needs. Diego not only considers your physical and mental interoception status, budget for each activity, but also anticipates your status changes and cost when you follow each event. 
        He is able to take into account <em>real</em> travel times from the <i>V-IRL</i> platform and select suitable dining options by <u class="colul">collaborating</u> with another 
        restaurant recommendation agent.
        </p>
        <d-figure>
            <figure>
                <img src="virl_images/interactive_concierge.png" alt="Interactive Concierge Diego" style="width: 130%">
                <figcaption style="width: 130%"><em>The Perfect Day Itinerary</em>: Crafted by Diego, our iterative concierge agent, this schedule is meticulously tailored, accounting for your mental and physical well-being and budget variations as your day unfolds.</figcaption>
            </figure>
        </d-figure>
        <p>
            As shown in the following figure, you can intervene Diego's planning process by adjusting your interoception status or providing verbal feedback for Diego.
            In response, Diego promptly revises his original plan to make it accommodate your demands, and re-estimate your state changes after revision.
        </p>
        <d-figure>
            <figure>
                <img src="virl_images/interactive_concierge_revise.png" alt="Interactive Concierge Diego Revise" style="width: 130%">
                <figcaption style="width: 130%; text-align: center">Diego adapts original plan to suit user's intervention.</figcaption>
            </figure>
        </d-figure>

        <p>
            Additionally, grounded on tightly related street views and <u class="geoul">Map</u> in <i>V-IRL</i>, Diego travels places in his itinerary to scout for potential scenic viewpoints for you as shown in the following figure. 
            He uses <u class="cvul">VQA</u> to assess each captured views, attaching highly rated positions to your itinerary.
        </p>
        <d-figure>
            <figure>
                <img src="virl_images/photographer.png" alt="Interactive Concierge Diego takes photos" style="width: 130%">
                <figcaption style="width: 130%; text-align: center">Diego records attractive locations in your itinerary.</figcaption>
            </figure>
        </d-figure>
        

        <section id="system">
            <h2>System Fundamental</h2>
            <d-figure>
                <figure>
                    <img src="virl_images/architecture.png" alt="Architecture figure" style="width: 100%">
                    <figcaption style="width: 100%; text-align: center">Hierarchical <i>V-IRL</i> architecture.</figcaption>
                </figure>
            </d-figure>


            <d-figure>
                <figure>
                    <img src="virl_images/interactive_concierge_pipeline.png" alt="Pipeline of Interactive Concierge agent" style="width: 100%">
                    <figcaption style="width: 100%; text-align: center">Pipeline overview of interactive concierge agent Diego.</figcaption>
                </figure>
            </d-figure>
        </section>
        

        <section id="virlbenchmark">
            <h2><em>V-IRL</em> Benchmark</h2>
            <p>
                The essential attributes of <i>V-IRL</i> include its ability to access geographically diverse data derived from real-world sensory input, and its API that facilitates interaction with Google Map Platform (GMP) <d-cite key="google_map_platform"></d-cite>. 
                This enables us to develop three <i>V-IRL</i> benchmarks to assess the capabilities of existing vision models in such open-world data distribution.
            </p>
            <h3><em>V-IRL Place:</em> Localization</h3>
            <p>
                Every day, humans traverse through cities, moving between diverse places to fulfil a range of goals, like the Intentional Explorer agent.
                We assess the performance of vision models on the everyday human activity <i>place localization</i> using street view imagery and associated place data.
            </p>
            <h3><em>V-IRL Place:</em> Recognition and VQA</h3>
            <p>
                In contrast to the challenging <i>V-IRL</i> place localization task on street view imagery, in real life, humans can recognize businesses by taking a closer, place-centric look.
                In this regard, we assess existing vision models on two perception tasks based on place-centric images: <i>i)</i> recognizing specific place types; <i>ii)</i> identifying human intentions by Vision Question Answering (VQA), named intention VQA.
            </p>
            <h3><em>V-IRL</em> Vision Language Navigation</h3>
            <p>
                As discussed in the <i>V-IRL</i> agents section, Intentional Explorer and Tourist agents require collaboration between vision models and language models to accomplish complex tasks. Therefore, this motivates us to investigate the performance of vision-language collaboration, with environmental information acquired through visual perception models from real-world images. 
                This prompts us to build an embodied task for jointly leveraging vision and language models along with the realistic street views in <i>V-IRL</i>. 
                In this regard, we build this <i>V-IRL</i> Vision Language Navigation (VLN) benchmark.
            </p>
            <h3><em>Geographic Diversity</h3>
            <p>
                Spanning 12 cities across the globe, our <i>V-IRL</i> benchmarks provide an opportunity to analyze the inherent model biases in different regions. As depicted in the following figure, vision models demonstrate subpar performance on all three benchmark tasks in Lagos, Tokyo, Hong Kong, and Buenos Aires.
                In Lagos, vision models might struggle due to its non-traditional street views relative to more developed cities (see street views in \cref{fig:teaser}). For cities like Tokyo, Hong Kong and Buenos Aires, an intriguing observation is their primary use of non-English languages in street views.
                This suggests that existing vision models face challenges with multilingual image data.
            </p>
            <d-figure>
                <figure>
                    <img src="virl_images/bm_city_analysis.png" alt="City level analysis" style="width: 100%">
                    <figcaption style="width: 100%">City-level visualization of <i>V-IRL</i> benchmark results.</figcaption>
                </figure>
            </d-figure>

        </section>
        
        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{VIRL,<br>
                &nbsp;&nbsp;title={V-IRL: Grounding Virtual Intelligence in Real Life},<br>
                &nbsp;&nbsp;author={Yang, Jihan and Ding, Runyu and Brown, Ellis and Qi, Xiaojuan and Xie, Saining},<br>
                &nbsp;&nbsp;year={2023},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2312},<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
          </d-appendix>
          
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

          <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>

        <!-- <script type="text/bibliography">
        
        </script> -->
        <script src="contents_bar.js"></script>
        
        

    </body>
</html>
